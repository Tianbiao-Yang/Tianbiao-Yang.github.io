---
layout:     post                 # 使用的布局（不需要改）
title:      Seq2Seq               # 标题 
subtitle:   Neural Sequence-toSequence Models #副标题
date:       2019-02-19              # 时间
author:     Yichen Yang                      # 作者
header-img: img/post-bg-halting.jpg  #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - Machine Learning
    - Deep Learning
    - Paper
---
# Seq2Seq（ Neural Sequence-toSequence Models）
## 摘要
。。。。。。。。。。。。
## 概念
一种通用的编码器——解码器框架，可用于机器翻译、文本摘要、会话建模、图像字幕等场景中，只要我们的任务，可以将输入数据以一种格式编码并将其以另一种格式解码，我们就可以使用或者扩展这个框架。
## 实施
1. 导入必须的python包和选择设备 

```python
from __future__ import unicode_literals, print_function, division
#在老版本的Python中兼顾新特性的一种方法
from io import open
import unicodedata
import string
import re
import random

import torch
import torch.nn as nn
from torch import optim
import torch.nn.functional as F

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
#检验是否可用GPU，不行选用CPU设备
```

* `from __future__ import **`
  * Python提供了__future__模块，为了在老版本的Python中兼顾新特性的一种方法
  * 例如：在开头加上from __future__ import print_function，在python2.7下使用print可以像python3.6那样加括号。
  * division： 整数除法； unicode_literals： 字符串文本成为Unicode字符串
* `torch.device()`
  * 在PyTorch中使用GPU是非常简单的，你可以将模型放到一块GPU上：
  
  ```python
  device = torch.device("cuda:0")
  model.to(device)
  ```

  * 然后，你可以将所有的tensors复制到GPU上(返回的是一个备份而不是重写原来的tensors)：
  
  ```python
  mytensor = my_tensor.to(device)
  ```
  
1. 数据预处理
```python

```